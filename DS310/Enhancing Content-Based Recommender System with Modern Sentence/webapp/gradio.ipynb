{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzXPJQ7HyFbY"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install git+https://github.com/UKPLab/sentence-transformers.git\n",
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1up0LMPryVEu"
      },
      "outputs": [],
      "source": [
        "# import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FguynwlFpVIE"
      },
      "outputs": [],
      "source": [
        "meta_data = pd.read_csv(\"/content/drive/MyDrive/DS310_Final/meta_final.csv\")\n",
        "# meta_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-y_HvPM_pbPF"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class SentenceTransformerBackend():\n",
        "\n",
        "    def __init__(self, embedding_model: Union[str, SentenceTransformer]):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(embedding_model, SentenceTransformer):\n",
        "            self.embedding_model = embedding_model\n",
        "        elif isinstance(embedding_model, str):\n",
        "            self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        else:\n",
        "            raise ValueError(\"Please select a correct SentenceTransformers model: \\n\"\n",
        "                             \"`from sentence_transformers import SentenceTransformer` \\n\"\n",
        "                             \"`model = SentenceTransformer('all-MiniLM-L6-v2')`\")\n",
        "\n",
        "    def embed(self,\n",
        "              documents: List[str],\n",
        "              verbose: bool = False) -> np.ndarray:\n",
        "        embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
        "        embeddings = torch.from_numpy(embeddings)\n",
        "        self.embed_matrix = embeddings\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgbumBIMPjcV",
        "outputId": "9df46bf7-71f7-4a6b-928a-81a4eb3619a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "MIN_WORDS = 4\n",
        "MAX_WORDS = 200\n",
        "\n",
        "\n",
        "class Word2Vec_SenEmbed():\n",
        "    def __init__(self, stopwords=STOPWORDS, min_df=5, max_df=0.95, max_features=8000, word2vec_model=None, path=None):\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.max_features = max_features\n",
        "        self.stopwords = stopwords\n",
        "        # if not word2vec_model:\n",
        "        #     self.word2vec = word2vec_model\n",
        "        # else:\n",
        "        #     self.load_embeddings_matrix(path)\n",
        "        EMBEDDING_FILE = '/content/drive/MyDrive/DS310_Final/GoogleNews-vectors-negative300.bin.gz'\n",
        "        self.google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "    def embed(self, corpus, verbose=False):\n",
        "        token_stop = self.tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
        "        corpus = [self.tokenizer(sentence) for sentence in corpus]\n",
        "        embedding_matrices = []\n",
        "        for sentence in corpus:\n",
        "            embedding_matrix = self.sentence_embedding(sentence, self.google_word2vec)\n",
        "            embedding_matrices.append(embedding_matrix)\n",
        "\n",
        "        embedding_matrices = np.array(embedding_matrices)\n",
        "        embed_matrix_tensor = torch.from_numpy(embedding_matrices)\n",
        "        self.embed_matrix = embed_matrix_tensor\n",
        "        return embed_matrix_tensor\n",
        "\n",
        "    # def load_embeddings_matrix(self, path):\n",
        "    #     with open(path, 'rb') as f:\n",
        "    #         embed_matrix = pickle.load(f)\n",
        "    #         self.embed_matrix = torch.tensor(embed_matrix, dtype=float)\n",
        "\n",
        "    def tokenizer(self, sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
        "        if lemmatize:\n",
        "            stemmer = WordNetLemmatizer()\n",
        "            tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
        "        else:\n",
        "            tokens = [w for w in word_tokenize(sentence)]\n",
        "        token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
        "                                                            and w not in stopwords)]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def sentence_embedding(self, sentence, word2vec_model):\n",
        "    # Calculate the mean of word vectors for words present in the Word2Vec model\n",
        "        return np.mean([word2vec_model[word] for word in sentence if word in word2vec_model], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI_l_QTfHuEh",
        "outputId": "4c4cead6-a745-4ce3-e871-f2a1c134ec1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "MIN_WORDS = 4\n",
        "MAX_WORDS = 200\n",
        "\n",
        "\n",
        "class TFIDF_SenEmbed():\n",
        "    def __init__(self, stopwords=STOPWORDS, min_df=5, max_df=0.95, max_features=8000):\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "        self.max_features = max_features\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def embed(self, corpus, verbose=False):\n",
        "        token_stop = self.tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
        "        self.vectorizer = TfidfVectorizer(stop_words=token_stop,\n",
        "                                     tokenizer=self.tokenizer,\n",
        "                                     min_df=self.min_df,\n",
        "                                     max_df=self.max_df,\n",
        "                                     max_features=self.max_features)\n",
        "\n",
        "        embed_matrix = self.vectorizer.fit_transform(corpus)\n",
        "        embed_matrix_dense = embed_matrix.toarray()\n",
        "        embed_matrix_tensor = torch.from_numpy(embed_matrix_dense)\n",
        "        self.embed_matrix = embed_matrix_tensor\n",
        "        return embed_matrix_tensor\n",
        "\n",
        "    def tfidf_transform(self):\n",
        "      return self.vectorizer\n",
        "\n",
        "    # def load_embeddings_matrix(self, path):\n",
        "    #     with open(path, 'rb') as f:\n",
        "    #         embed_matrix = pickle.load(f)\n",
        "    #         self.embed_matrix = torch.tensor(embed_matrix, dtype=float)\n",
        "\n",
        "    def tokenizer(self, sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
        "        if lemmatize:\n",
        "            stemmer = WordNetLemmatizer()\n",
        "            tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
        "        else:\n",
        "            tokens = [w for w in word_tokenize(sentence)]\n",
        "        token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
        "                                                            and w not in stopwords)]\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MwjXfGWrSgSa"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/DS310_Final/embeddings/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer_tfidf = pickle.load(f)\n",
        "word2vec_model = Word2Vec_SenEmbed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Gradio app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6wNd9dLAv3yP"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import os\n",
        "import base64\n",
        "\n",
        "# Replace these with your actual embedding model and metadata\n",
        "# Assuming ST5 is an instance of your embedding model\n",
        "\n",
        "def embed_query(query, model):\n",
        "    if not isinstance(query, list):\n",
        "        query = [query]\n",
        "    query = [q.lower() for q in query]\n",
        "\n",
        "    if model.__class__.__name__ == 'TfidfVectorizer':\n",
        "      # print(\"helloooooo\")\n",
        "      query_embedding = torch.from_numpy(model.transform(query).toarray())\n",
        "    else:\n",
        "      query_embedding = model.embed(query)\n",
        "    return query_embedding\n",
        "\n",
        "\n",
        "def get_metadata(idx):\n",
        "    return meta_data.iloc[idx]\n",
        "\n",
        "def extract_best_indices(m, topk, mask=None):\n",
        "    if len(m.shape) > 1:\n",
        "        cos_sim = np.mean(m, axis=0)\n",
        "    else:\n",
        "        cos_sim = m\n",
        "    index = np.argsort(cos_sim)[::-1]\n",
        "    if mask is not None:\n",
        "        assert mask.shape == m.shape\n",
        "        mask = mask[index]\n",
        "    else:\n",
        "        mask = np.ones(len(cos_sim))\n",
        "    mask = np.logical_or(cos_sim[index] != 0, mask)\n",
        "    best_index = index[mask][:topk]\n",
        "\n",
        "    # Return a list of tuples containing index and similarity score\n",
        "    result = [(idx, cos_sim[idx]) for idx in best_index]\n",
        "    return result\n",
        "\n",
        "def choice_model(name_model):\n",
        "  name_model = str(name_model)\n",
        "  file_path = \"/content/drive/MyDrive/DS310_Final/embeddings/\"\n",
        "\n",
        "  if name_model == 'sentence-transformers/sentence-t5-base':\n",
        "      file_path += \"sentence_t5_base.pkl\"\n",
        "  elif name_model == 'Muennighoff/SGPT-125M-weightedmean-nli-bitfit':\n",
        "      file_path += \"sgpt.pkl\"\n",
        "  else:\n",
        "      file_path += name_model + '.pkl'\n",
        "\n",
        "  with open(file_path, 'rb') as handle:\n",
        "    embed_matrix = pickle.load(handle)\n",
        "\n",
        "  if name_model != 'word2vec' and name_model != 'tfidf':\n",
        "    model = SentenceTransformerBackend(name_model)\n",
        "    model.embedding_model.max_seq_length = 512\n",
        "  elif name_model == 'tfidf':\n",
        "    model = vectorizer_tfidf\n",
        "  else:\n",
        "      model = word2vec_model\n",
        "\n",
        "  return model, embed_matrix\n",
        "\n",
        "\n",
        "def image_to_markdown(image):\n",
        "    # Convert image to RGB mode\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return f\"![Image](data:image/png;base64,{encoded_image})\"\n",
        "\n",
        "def predict(*args):\n",
        "\n",
        "    query = args[:-2]\n",
        "    query = list(query)\n",
        "    slider = args[-2]\n",
        "    name_model = str(args[-1])\n",
        "    # print(query_embedding)\n",
        "    # print(slider)\n",
        "    # print(name_model)\n",
        "    query = query[:int(slider)]\n",
        "    model, embed_matrix = choice_model(name_model)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    query_embeddings = embed_query(query, model)\n",
        "    query_embeddings = torch.mean(query_embeddings, dim=0).reshape(1, -1)\n",
        "    cos_sim = cosine_similarity(query_embeddings, embed_matrix)\n",
        "    topk_indices = extract_best_indices(cos_sim, topk=30)  # Assuming you have a function named extract_best_indices\n",
        "\n",
        "    run_time = time.time() - start_time\n",
        "    run_time = float(\"{:.2f}\".format(run_time))\n",
        "\n",
        "    # Prepare output\n",
        "    results = []\n",
        "    for index, score in topk_indices:\n",
        "        metadata = get_metadata(index)\n",
        "        title = metadata['title']\n",
        "        author_name = metadata['author_name']\n",
        "        description = metadata['description']  # Truncate description if too long\n",
        "        rating = metadata['average_rating']\n",
        "        image_url = metadata['image_url']\n",
        "        response = requests.get(image_url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        # temp_image_path = \"temp_image.png\"\n",
        "        # img.save(temp_image_path)\n",
        "\n",
        "      # Convert image to Markdown\n",
        "        image_markdown = image_to_markdown(img)\n",
        "\n",
        "      # Remove the temporary image\n",
        "        # os.remove(temp_image_path)\n",
        "        results.append([image_markdown, title, author_name, description, rating, score])\n",
        "\n",
        "\n",
        "    return results, run_time\n",
        "\n",
        "  # Gradio Interface\n",
        "\n",
        "def variable_inputs(k):\n",
        "  k = int(k)\n",
        "  return [gr.Textbox(visible=True)]*k + [gr.Textbox(visible=False)]*(10-k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "cqyKyJZNW-nu",
        "outputId": "49fa0539-7240-4b12-b56a-dbe0954b5b6d"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as iface:\n",
        "  with gr.Row():\n",
        "    # input = gr.Textbox(placeholder=\"Enter query here\", label=\"Query\")\n",
        "    models = gr.Dropdown(['tfidf','word2vec', 'bert-base-uncased', 'bert-base-multilingual-uncased', 'sentence-transformers/sentence-t5-base', 'Muennighoff/SGPT-125M-weightedmean-nli-bitfit'], label=\"Embedding Model\")\n",
        "    button = gr.Button(\"Search\")\n",
        "    slider = gr.Slider(minimum=0, maximum=10, value=0, step=1, label=\"user items\")\n",
        "    inputs = []\n",
        "    for i in range(10):\n",
        "        label = f\"Query/Item {i+1}\"\n",
        "        t = gr.Textbox(label=label, visible=False)\n",
        "        inputs.append(t)\n",
        "    slider.change(variable_inputs, inputs=slider, outputs=inputs)\n",
        "  with gr.Row():\n",
        "    run_time = gr.Number(label=\"Run Time\")\n",
        "\n",
        "  with gr.Row():\n",
        "    output = gr.Dataframe(\n",
        "        headers=['Image', \"Title\", \"Author\", \"Description\", \"Rating\", \"Score\"],\n",
        "        datatype=['markdown', \"str\", \"str\", \"str\", \"number\", \"number\"],\n",
        "        wrap=True,\n",
        "        interactive=True\n",
        "    )\n",
        "\n",
        "\n",
        "  button.click(fn=predict, inputs=[*inputs, slider, models], outputs=[output, run_time])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(show_api=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
