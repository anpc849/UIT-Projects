{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "import random\n",
    "\n",
    "import json\n",
    "import time\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.autograd import Variable\n",
    "# from keras.optimizers import Adam\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "from tiktok_downloader import snaptik\n",
    "import os\n",
    "import torch\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "import pytorchvideo.data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, json_tuple, udf, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/teamspace/studios/this_studio/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/zeus/.ivy2/cache\n",
      "The jars for the packages stored in: /home/zeus/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-68a14eaa-db12-491d-8f21-dc8b78cde6f9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.7.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-6 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      ":: resolution report :: resolve 1296ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-6 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.7.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.7.0] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   3   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-68a14eaa-db12-491d-8f21-dc8b78cde6f9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n",
      "24/07/05 05:21:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-192-12-244.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-TikTokVideos</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f53ec2cbb20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.5.1' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.7.0' #your kafka version\n",
    "]\n",
    "    # spark = SparkSession.builder.master(\"local\").appName(\"TikTokVideos\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "    # spark\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-TikTokVideos\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.python.worker.reuse\",True) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", True) \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model & Spark streaming with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'Adult Content', 1: 'Harmful Content', 2: 'Safe', 3: 'Suicide'}\n",
    "label2id = {'Adult Content': 0, 'Harmful Content': 1, 'Safe': 2, 'Suicide': 3}\n",
    "device = \"cpu\"\n",
    "model_ckpt = \"/teamspace/studios/this_studio/checkpoint_model/timesformer-base-finetuned-k400-finetuned-ucf101-subset/checkpoint-2300\"\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(16),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize((224, 224)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "    #image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt, attn_implementation=\"sdpa\")\n",
    "model = TimesformerForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_label(topic_name, spark_df):\n",
    "    def collate_fn(examples):\n",
    "        # permute to (num_frames, num_channels, height, width)\n",
    "        pixel_values = torch.stack(\n",
    "             [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "        )\n",
    "        return {\"pixel_values\": pixel_values}\n",
    "\n",
    "    def predict_batch_fn():\n",
    "\n",
    "        import pytorchvideo.data\n",
    "        import numpy as np\n",
    "        import random\n",
    "        import os\n",
    "        import socket\n",
    "        import shutil\n",
    "        \n",
    "        def preprocess_batch(tiktok_urls):\n",
    "\n",
    "            idx_folder = socket.gethostname() + str(random.randint(1, 100))\n",
    "            download_folder = f\"/teamspace/studios/this_studio/KidGuardScan/video_predict/{idx_folder}/noclass\"\n",
    "            os.makedirs(download_folder, exist_ok=True)\n",
    "            \n",
    "            for index, url in enumerate(tiktok_urls):\n",
    "                d = snaptik(url)\n",
    "                parts = url.split(\"/\")\n",
    "                username = parts[3][1:]\n",
    "                video_id = parts[5]\n",
    "                name_video = f\"{index}.mp4\"\n",
    "                # download_folder = (\n",
    "                #     \"/teamspace/studios/this_studio/Doan/video_predict/{idx_folder}/noclass\"\n",
    "                # )\n",
    "                download_path = os.path.join(download_folder, name_video)\n",
    "                # if os.path.exists(download_path):\n",
    "                #     continue\n",
    "                try:\n",
    "                    d[0].download(download_path)\n",
    "                except:\n",
    "                    print(\"Error: \", url)\n",
    "\n",
    "            val_dataset = pytorchvideo.data.Ucf101(\n",
    "                data_path=os.path.join(\n",
    "                    \"/teamspace/studios/this_studio/KidGuardScan/video_predict\", str(idx_folder)\n",
    "                ),\n",
    "                clip_sampler=pytorchvideo.data.make_clip_sampler(\n",
    "                    \"random\", 2),  # Note the positional arguments\n",
    "                decode_audio=False,\n",
    "                transform=val_transform,\n",
    "                video_sampler=torch.utils.data.SequentialSampler,\n",
    "            )\n",
    "            return val_dataset, idx_folder\n",
    "\n",
    "        def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "            decoded_urls = [url.decode(\"utf-8\") for url in inputs]\n",
    "            inputs_dataset, idx_folder = preprocess_batch(decoded_urls)\n",
    "            size_prediction = len(inputs)\n",
    "            dataloader = DataLoader(\n",
    "                inputs_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn\n",
    "            )\n",
    "            predictions = []\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                if i == size_prediction:\n",
    "                    break\n",
    "\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(pixel_values)\n",
    "                    logits = outputs.logits  # [batch_size, num_classes]\n",
    "                    prediction = torch.argmax(logits, dim=-1).item()\n",
    "                predictions.append(prediction)\n",
    "\n",
    "            base_folder = \"/teamspace/studios/this_studio/KidGuardScan/video_predict/\"\n",
    "            shutil.rmtree(os.path.join(base_folder, idx_folder))\n",
    "\n",
    "            return np.array(predictions)\n",
    "\n",
    "        return predict\n",
    "\n",
    "    from pyspark.ml.functions import predict_batch_udf\n",
    "    from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "    predict_fn_for_model = predict_batch_udf(predict_batch_fn,\n",
    "                                    return_type=IntegerType(),\n",
    "                                    batch_size=3,\n",
    "    )\n",
    "\n",
    "    result = spark_df.withColumn(\"predicted_label_id\", predict_fn_for_model(col(\"encoded_urls\"))) \\\n",
    "                    .withColumn(\"predicted_label\", when(col(\"predicted_label_id\") == 0, \"Adult Content\")\n",
    "                                   .when(col(\"predicted_label_id\") == 1, \"Harmful Content\")\n",
    "                                   .when(col(\"predicted_label_id\") == 2, \"Safe\")\n",
    "                                   .when(col(\"predicted_label_id\") == 3, \"Suicide\")\n",
    "                                   .otherwise(\"Unknown\"))  # Handle unexpected values\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kafka setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"Test_with_MongoDB11\"\n",
    "kafka_server = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaDf = spark.readStream.format('kafka').option('kafka.bootstrap.servers', kafka_server).option('subscribe', topic_name).option('startingOffsets', 'latest').load()\n",
    "kafkaDf = kafkaDf.withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "kafkaDf = kafkaDf.withColumn(\"key\", col(\"key\").cast(\"string\"))\n",
    "# kafkaDf = kafkaDf.filter(col(\"key\").isin(hashtag))\n",
    "kafkaDf = kafkaDf.withColumn(\"tiktok_url\",\n",
    "    expr(\"concat('https://www.tiktok.com/@', get_json_object(value, '$.author.uniqueId'), '/video/', get_json_object(value, '$.id'))\"))\n",
    "kafkaDf = kafkaDf.withColumn(\"encoded_urls\", expr(\"unhex(hex(tiktok_url))\"))\n",
    "\n",
    "kafkaDf = prediction_label(topic_name, kafkaDf)\n",
    "\n",
    "value_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"desc\", StringType(), True),\n",
    "    StructField(\"author\", StructType([\n",
    "        StructField(\"uniqueId\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"textExtra\", ArrayType(MapType(StringType(), StringType())), True),\n",
    "    StructField(\"stats\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"diversificationLabels\", ArrayType(StringType()), True),\n",
    "    StructField(\"suggestedWords\", ArrayType(StringType()), True),\n",
    "    StructField(\"createTime\", StringType(), True),\n",
    "    StructField(\"locationCreated\", StringType(), True),\n",
    "    StructField(\"video\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Extract the JSON fields from the 'value' column\n",
    "kafkaDf = kafkaDf.withColumn(\"value_json\", from_json(col(\"value\"), value_schema))\n",
    "\n",
    "# Define a UDF to concatenate hashtags\n",
    "@udf(returnType=StringType())\n",
    "def concatenate_hashtags(textExtra):\n",
    "    if textExtra is None:\n",
    "        return \"\"\n",
    "    return \" \".join([item['hashtagName'] for item in textExtra if item['hashtagName']])\n",
    "\n",
    "# Create the new DataFrame with the required columns\n",
    "kafkaDf_predict = kafkaDf.select(\n",
    "    col(\"value_json.id\").alias(\"id\"),\n",
    "    col(\"value_json.desc\").alias(\"description\"),\n",
    "    col(\"value_json.author.uniqueId\").alias(\"user_id\"),\n",
    "    concatenate_hashtags(col(\"value_json.textExtra\")).alias(\"hashtags\"),\n",
    "    col(\"value_json.stats\").alias(\"stats\"),\n",
    "    col(\"value_json.diversificationLabels\").alias(\"diversificationLabels\"),\n",
    "    col(\"value_json.suggestedWords\").alias(\"suggestedWords\"),\n",
    "    col(\"value_json.createTime\").alias(\"created_at\"),\n",
    "    col(\"value_json.video.duration\").alias(\"duration\"),\n",
    "    col(\"value_json.locationCreated\").alias(\"locationCreated\"),\n",
    "    col(\"tiktok_url\"),\n",
    "    # col(\"encoded_urls\"),\n",
    "    # col(\"key\"),\n",
    "    col(\"predicted_label_id\"),\n",
    "    col(\"predicted_label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming setup and write the result (predicted label and metadata videos) to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KidGuardScan/uri_mongodb.txt', 'r') as file:\n",
    "    uri = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "\n",
    "# Prompt the user to enter the MongoDB URI\n",
    "# uri = getpass(\"Enter the MongoDB URI: \")  ##read from files, ...\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['VideoTikTok']\n",
    "collection = db['videos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mongo(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        print(f\"Batch {epoch_id} is empty. No records to write to MongoDB.\")\n",
    "        return\n",
    "            \n",
    "    data = df.toJSON().collect()\n",
    "    data = [json.loads(doc) for doc in data]\n",
    "    \n",
    "    for doc in data:\n",
    "        doc['_id'] = doc.pop('id')\n",
    "    \n",
    "    collection.insert_many(data)\n",
    "    # client.close()\n",
    "    print(f\"Batch {epoch_id} written to MongoDB successfully with {len(data)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 05:23:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4af7e4ae-718c-4fd1-9b93-deb5839cc243. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/05 05:23:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is empty. No records to write to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "q1name = \"prediction\"\n",
    "stream_writer1 = kafkaDf_predict.writeStream.queryName(q1name).trigger(processingTime=\"120 seconds\").outputMode(\"append\").foreachBatch(write_to_mongo).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 05:28:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/07/05 05:28:03 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/07/05 05:28:03 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/07/05 05:28:52 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/07/05 05:28:53 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/07/05 05:28:53 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 written to MongoDB successfully with 3 records.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stream_writer1.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
